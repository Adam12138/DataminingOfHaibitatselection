package GradientBoostingTree

import breeze.numerics._
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.tree.GradientBoostedTrees
import org.apache.spark.mllib.tree.configuration.BoostingStrategy
import org.apache.spark.mllib.tree.model.{DecisionTreeModel, Node}
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable.ArrayBuffer

/**
 * Created by machaoyi on 16/10/11.
 */
object GrandientBoostingTreeJob {
  def ModeltoString(topnode: Node) : String = {
    var leftid = "None"
    var rightid = "None"
    var feature = "None"
    //var featureType = "None"
    var threshold = "None"
    if (topnode.isLeaf == false){
      feature = topnode.split.get.feature.toString
      threshold = topnode.split.get.threshold.toString
    }
    if (topnode.leftNode != None){
      leftid = topnode.leftNode.get.id.toString
    }
    if (topnode.rightNode != None){
      rightid = topnode.rightNode.get.id.toString
    }

    var mess : String = topnode.id.toString+","+leftid+","+
      rightid+","+topnode.isLeaf.toString+","+topnode.predict.predict.toString+
      ","+feature+","+threshold
    var leftmess : String = ""
    var rightmess : String = ""
    if (topnode.leftNode != None){
      leftmess = ModeltoString(topnode.leftNode.get)
    }
    if (topnode.rightNode != None){
      rightmess = ModeltoString(topnode.rightNode.get)
    }
    var result : String = mess
    if(!leftmess.equals(""))result+= " "+leftmess
    if(!rightmess.equals(""))result+= " "+rightmess

    return result
  }
  def predict( testdata : Array[Double] , forest : Map[Int,Cart], step : Int) : Double = {
    //val testdata = a.split(" ")
    var result = 0.0
    for (i <- 0  until step) {
      val tree = forest(i).carttree
      val treerate = forest(i).rate
      var index : Int = 0
      while(tree(index).leaf == false){
        val splitindex = tree(index).splitpos
        val splitoutput = tree(index).splitoutput
        if(testdata(splitindex).toDouble < splitoutput)
          index = tree(index).left - 1
        else
          index = tree(index).right - 1
      }
      result += tree(index).output * treerate
    }

    return result
  }
  def main(args:Array[String]) = {
    val sc: SparkContext = new SparkContext(new SparkConf());
    val hiveContext = new HiveContext(sc)
    import hiveContext.implicits._
    val modelfilter = args(0)
    val testfilter =  args(1)
    val trainClause = " dt='"+modelfilter+"' "
    val testClause = " dt='" +testfilter+"' "
    val traintable = args(2)
    val testtable = args(3)
    val treenum = args(4).toInt
    val depth = args(5).toInt
    val data_src =  hiveContext.sql(s"""
                    select
                    *
                    from
                    $traintable
                    where $trainClause
        """.stripMargin)
    val data_test =  hiveContext.sql(s"""
                    select
                    *
                    from
                     $testtable
                    where $testClause
        """.stripMargin)
    val trainData  = data_src.map(r => (
      r.getDouble(4), // Get target value
      // Map feature indices to values
      r.getInt(1),r.getDouble(2),r.getDouble(5),r.getDouble(6),r.getDouble(7),r.getDouble(8),r.getDouble(11)
      ,r.getDouble(12),r.getInt(13),r.getDouble(14),r.getDouble(15),r.getDouble(16),r.getDouble(17),r.getDouble(18)
      ,r.getDouble(19),r.getDouble(20),r.getDouble(21),r.getDouble(22),r.getString(0)
      )).flatMap(
      l => {
        var a :String = ""
        a += l._1+", "+l._2+" "+l._3+" "+l._4+" "+l._5+" "+l._6+" "+l._7+" "+l._8+" "+
          l._9+" "+l._10+" "+l._11+" "+l._12+" "+l._13+" "+l._14+" "+l._15+" "+l._16+" "+l._17+" "+l._18+" "+l._19
        var data : LabeledPoint = LabeledPoint.parse(a)

        val bm_waybill_id = l._20.toString
        var ret = ArrayBuffer[(String,LabeledPoint)]()
        ret += ((bm_waybill_id,data))
        ret.iterator
      }
    )
    val splits = trainData.randomSplit(Array(0.9, 0.1))
    val (trainingData, testData) = (splits(0), splits(1))
//    val testData  = data_test.map(r => (
//      r.getString(0) , r.getDouble(4), // Get target value
//      // Map feature indices to values
//      r.getInt(1),r.getDouble(2),r.getDouble(5),r.getDouble(6),r.getDouble(7),r.getDouble(8),r.getDouble(11)
//      ,r.getDouble(12),r.getInt(13),r.getDouble(14),r.getDouble(15),r.getDouble(16),r.getDouble(17),r.getDouble(18)
//      ,r.getDouble(19),r.getDouble(20),r.getDouble(21),r.getDouble(22)
//      )).flatMap(
//      l => {
//        var a :String = ""
//        a += l._2+", "+l._3+" "+l._4+" "+l._5+" "+l._6+" "+l._7+" "+l._8+" "+
//          l._9+" "+l._10+" "+l._11+" "+l._12+" "+l._13+" "+l._14+" "+l._15+" "+l._16+" "+l._17+" "+l._18+" "+l._19+" "+l._20
//        var data : LabeledPoint = LabeledPoint.parse(a)
//        val bm_waybill_id = l._1
//
//        var rew = ArrayBuffer[(String,LabeledPoint)]()
//        rew += ((bm_waybill_id,data))
//        rew.iterator
//      }
//    )
    // Train a GradientBoostedTrees model.
    // The defaultParams for Regression use SquaredError by default.
    var boostingStrategy = BoostingStrategy.defaultParams("Regression")
    boostingStrategy.setNumIterations(treenum)
    boostingStrategy.setLearningRate(0.1)
    boostingStrategy.treeStrategy.setMaxDepth(depth)
    boostingStrategy.treeStrategy.setMaxBins(32)



    //val categoricalFeaturesInfo = new java.util.Map[java.lang.Integer, java.lang.Integer]
    //boostingStrategy.treeStrategy.setCategoricalFeaturesInfo( categoricalFeaturesInfo)
    // Empty categoricalFeaturesInfo indicates all features are continuous.
    val model = GradientBoostedTrees.train(trainingData.map(r=>(r._2)), boostingStrategy)
    val modelList : Array[DecisionTreeModel] = model.trees
    var res = ArrayBuffer[(Int,String,Double,String)]()
    var index = 0
    for (i <- modelList){
      var learnrate = 0.1
      if (index == 0)
        learnrate = 1.0
      val message = ModeltoString(i.topNode)
      res += ((index,message,learnrate,modelfilter))
      index = index + 1
    }

    val rdd = sc.parallelize(res);
    // 注册临时表
    rdd.toDF("id", "model","learnrate", "dt").registerTempTable("bm_waybill_rf_spark_gbdt")

    // 写入天分区表
    hiveContext.sql("use mart_peisongpa")
    hiveContext.sql("set hive.exec.dynamic.partition=true")
    hiveContext.sql("set hive.exec.dynamic.partition.mode=nostrick")

    hiveContext.sql(
      """insert overwrite table mart_peisongpa.bm_waybill_rf_spark_gbdt partition(dt)
        |select id,model,learnrate,dt from bm_waybill_rf_spark_gbdt distribute by dt""".stripMargin)
//    val TestResult = testData.flatMap(
//      l =>{
//        val bm_waybill_id  = l._1
//        val data = l._2
//        var result = ArrayBuffer[(String,Double,String)]()
//        val deliveredtime = model.predict(data.features)
//        result+=((bm_waybill_id,deliveredtime,testfilter))
//        result.iterator
//      }
//    )
//    TestResult.toDF("bm_waybill_id","deliveredtime","dt").registerTempTable("bm_waybill_rf_spark_gbdt_Mllibtest")
//    // 写入天分区表
//    hiveContext.sql("use mart_peisongpa")
//    hiveContext.sql("set hive.exec.dynamic.partition=true")
//    hiveContext.sql("set hive.exec.dynamic.partition.mode=nostrick")
//
//    hiveContext.sql(
//      """insert overwrite table mart_peisongpa.bm_waybill_rf_spark_gbdt_Mllibtest partition(dt)
//        |select bm_waybill_id,deliveredtime,dt from bm_waybill_rf_spark_gbdt_Mllibtest distribute by dt""".stripMargin)


    //按树的数量计算计算误差
    val model_src =  hiveContext.sql(s"""
                    select
                    *
                    from
                    mart_peisongpa.bm_waybill_rf_spark_gbdt
                    where dt="20160928"
        """.stripMargin)
    //模型加载
    val modelstring = model_src.map(r=>(r.getInt(0),r.getString(1),r.getDouble(2)))
    val modelarray = modelstring.toArray()
    var forest : Map[Int,Cart] = Map()
    for (i <- modelarray){

      val nodepart = i._2.split(" ")
      //val maxindex : Int= (nodepart(nodepart.size-1).split(","))(0).toInt
      var cart : Array[Treenode] = new Array[Treenode](pow(2,depth+1))
      for (j <- nodepart){
        val part = j.split(",")
        val index = part(0).toInt
        val tempnode = new Treenode(part(0),part(1),part(2),part(3),part(4),part(5),part(6))
        //由于模型中节点索引从1开始，但数组中节点索引从0开始，所以需要转化
        cart(index-1) = tempnode
      }
      forest += (i._1 -> new Cart(cart,i._3))
    }
    val TestResult = testData.map(r=>(r._1,r._2.features.toArray))
    .flatMap(
      l => {
        var rew = ArrayBuffer[(String,Double,Int,String)]()

        val bm_waybill_id = l._1
        for( step <- 1 to modelarray.size){
          val deliveredtime = predict(l._2,forest,step)
          rew += ((bm_waybill_id,deliveredtime,step,modelfilter))
        }

        rew.iterator
      }
    )
    TestResult.toDF("bm_waybill_id","deliveredtime","step","dt").registerTempTable("bm_waybill_rf_spark_gbdt_step_test")
    // 写入天分区表
    hiveContext.sql("use mart_peisongpa")
    hiveContext.sql("set hive.exec.dynamic.partition=true")
    hiveContext.sql("set hive.exec.dynamic.partition.mode=nostrick")

    hiveContext.sql(
      """insert overwrite table mart_peisongpa.bm_waybill_rf_spark_gbdt_step_test partition(dt)
        |select bm_waybill_id,deliveredtime,step,dt from bm_waybill_rf_spark_gbdt_step_test distribute by dt""".stripMargin)
  }

}
